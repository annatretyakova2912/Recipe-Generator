{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60a9f2e",
   "metadata": {},
   "source": [
    "# Creating a Recipe Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddaed9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.10.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (31 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.25.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.10.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.20.3-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/python/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/python/lib/python3.13/site-packages (from torch) (80.10.2)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Downloading networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/python/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/python/lib/python3.13/site-packages (from torch) (2026.1.0)\n",
      "Collecting cuda-bindings==12.9.4 (from torch)\n",
      "  Downloading cuda_bindings-12.9.4-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.6 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.4.5 (from torch)\n",
      "  Downloading nvidia_nvshmem_cu12-3.4.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.6.0 (from torch)\n",
      "  Downloading triton-3.6.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting cuda-pathfinder~=1.1 (from cuda-bindings==12.9.4->torch)\n",
      "  Downloading cuda_pathfinder-1.3.3-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: numpy in /opt/python/lib/python3.13/site-packages (from torchvision) (2.4.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/python/lib/python3.13/site-packages (from torchvision) (12.1.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/python/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "Downloading torch-2.10.0-cp313-cp313-manylinux_2_28_x86_64.whl (915.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m915.7/915.7 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m  \u001b[33m0:00:10\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cuda_bindings-12.9.4-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (11.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.4.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (139.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.6.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (188.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cuda_pathfinder-1.3.3-py3-none-any.whl (27 kB)\n",
      "Downloading torchvision-0.25.0-cp313-cp313-manylinux_2_28_x86_64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.10.0-cp313-cp313-manylinux_2_28_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.20.3-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, filelock, cuda-pathfinder, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, cuda-bindings, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/25\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.29.2━━━━━━\u001b[0m \u001b[32m 6/25\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.29.2:━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/25\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.29.2━━━━━━━━━━━━\u001b[0m \u001b[32m 7/25\u001b[0m [nvidia-nccl-cu12]]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/25\u001b[0m [torchaudio]5\u001b[0m [torchaudio]]ver-cu12]2]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed cuda-bindings-12.9.4 cuda-pathfinder-1.3.3 filelock-3.20.3 mpmath-1.3.0 networkx-3.6.1 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.4.5 nvidia-nvtx-cu12-12.8.90 sympy-1.14.0 torch-2.10.0 torchaudio-2.10.0 torchvision-0.25.0 triton-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6521c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.12.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/python/lib/python3.13/site-packages (from tiktoken) (2026.1.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/python/lib/python3.13/site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/python/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/python/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/python/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken) (2026.1.4)\n",
      "Downloading tiktoken-0.12.0-cp313-cp313-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2750430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb0f03c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Architecture Hyperparameters ---\n",
    "block_size = 512       # Context window (long enough for a full recipe)\n",
    "n_embd = 384           # Embedding dimension\n",
    "n_head = 6             # Number of attention heads\n",
    "n_layer = 6            # Number of transformer blocks\n",
    "dropout = 0.2          # Higher dropout to prevent memorization of specific recipes\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "batch_size = 32        # How many recipes to process at once\n",
    "learning_rate = 3e-4   # The \"sweet spot\" for small transformers\n",
    "max_iters = 5000       # Total training steps\n",
    "eval_interval = 500    # How often to check validation loss\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# --- Special Control Tokens ---\n",
    "# These are the \"switches\" for your model\n",
    "VEGAN_TOKEN = \"[VEGAN]\"\n",
    "NORMAL_TOKEN = \"[NORMAL]\"\n",
    "TITLE_TOKEN = \"[TITLE]\"\n",
    "INGRED_TOKEN = \"[INGRED]\"\n",
    "STEPS_TOKEN = \"[STEPS]\"\n",
    "EOS_TOKEN = \"<|endoftext|>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e522bca",
   "metadata": {},
   "source": [
    "## 1). Data Pre-Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cc376e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 recipes...\n",
      "Processed 20000 recipes...\n",
      "Processed 30000 recipes...\n",
      "Processed 40000 recipes...\n",
      "Processed 50000 recipes...\n",
      "Processed 60000 recipes...\n",
      "Processed 70000 recipes...\n",
      "Processed 80000 recipes...\n",
      "Processed 90000 recipes...\n",
      "Processed 100000 recipes...\n",
      "Processed 110000 recipes...\n",
      "Processed 120000 recipes...\n",
      "Processed 130000 recipes...\n",
      "Processed 140000 recipes...\n",
      "Processed 150000 recipes...\n",
      "Processed 160000 recipes...\n",
      "Processed 170000 recipes...\n",
      "Processed 180000 recipes...\n",
      "Processed 190000 recipes...\n",
      "Processed 200000 recipes...\n",
      "Processed 210000 recipes...\n",
      "Processed 220000 recipes...\n",
      "Processed 230000 recipes...\n",
      "Processed 240000 recipes...\n",
      "Processed 250000 recipes...\n",
      "Processed 260000 recipes...\n",
      "Processed 270000 recipes...\n",
      "Processed 280000 recipes...\n",
      "Processed 290000 recipes...\n",
      "Processed 300000 recipes...\n",
      "Processed 310000 recipes...\n",
      "Processed 320000 recipes...\n",
      "Processed 330000 recipes...\n",
      "Processed 340000 recipes...\n",
      "Processed 350000 recipes...\n",
      "Processed 360000 recipes...\n",
      "Processed 370000 recipes...\n",
      "Processed 380000 recipes...\n",
      "Processed 390000 recipes...\n",
      "Processed 400000 recipes...\n",
      "Processed 410000 recipes...\n",
      "Processed 420000 recipes...\n",
      "Processed 430000 recipes...\n",
      "Processed 440000 recipes...\n",
      "Processed 450000 recipes...\n",
      "Processed 460000 recipes...\n",
      "Processed 470000 recipes...\n",
      "Processed 480000 recipes...\n",
      "Processed 490000 recipes...\n",
      "Processed 500000 recipes...\n",
      "Processed 510000 recipes...\n",
      "Processed 520000 recipes...\n",
      "Processed 530000 recipes...\n",
      "Processed 540000 recipes...\n",
      "Processed 550000 recipes...\n",
      "Processed 560000 recipes...\n",
      "Processed 570000 recipes...\n",
      "Processed 580000 recipes...\n",
      "Processed 590000 recipes...\n",
      "Processed 600000 recipes...\n",
      "Processed 610000 recipes...\n",
      "Processed 620000 recipes...\n",
      "Processed 630000 recipes...\n",
      "Processed 640000 recipes...\n",
      "Processed 650000 recipes...\n",
      "Processed 660000 recipes...\n",
      "Processed 670000 recipes...\n",
      "Processed 680000 recipes...\n",
      "Processed 690000 recipes...\n",
      "Processed 700000 recipes...\n",
      "Processed 710000 recipes...\n",
      "Processed 720000 recipes...\n",
      "Processed 730000 recipes...\n",
      "Processed 740000 recipes...\n",
      "Processed 750000 recipes...\n",
      "Processed 760000 recipes...\n",
      "Processed 770000 recipes...\n",
      "Processed 780000 recipes...\n",
      "Processed 790000 recipes...\n",
      "Processed 800000 recipes...\n",
      "Processed 810000 recipes...\n",
      "Processed 820000 recipes...\n",
      "Processed 830000 recipes...\n",
      "Processed 840000 recipes...\n",
      "Processed 850000 recipes...\n",
      "Processed 860000 recipes...\n",
      "Processed 870000 recipes...\n",
      "Processed 880000 recipes...\n",
      "Processed 890000 recipes...\n",
      "Processed 900000 recipes...\n",
      "Processed 910000 recipes...\n",
      "Processed 920000 recipes...\n",
      "Processed 930000 recipes...\n",
      "Processed 940000 recipes...\n",
      "Processed 950000 recipes...\n",
      "Processed 960000 recipes...\n",
      "Processed 970000 recipes...\n",
      "Processed 980000 recipes...\n",
      "Processed 990000 recipes...\n",
      "Processed 1000000 recipes...\n",
      "Processed 1010000 recipes...\n",
      "Processed 1020000 recipes...\n",
      "Processed 1030000 recipes...\n",
      "Processed 1040000 recipes...\n",
      "Processed 1050000 recipes...\n",
      "Processed 1060000 recipes...\n",
      "Processed 1070000 recipes...\n",
      "Processed 1080000 recipes...\n",
      "Processed 1090000 recipes...\n",
      "Processed 1100000 recipes...\n",
      "Processed 1110000 recipes...\n",
      "Processed 1120000 recipes...\n",
      "Processed 1130000 recipes...\n",
      "Processed 1140000 recipes...\n",
      "Processed 1150000 recipes...\n",
      "Processed 1160000 recipes...\n",
      "Processed 1170000 recipes...\n",
      "Processed 1180000 recipes...\n",
      "Processed 1190000 recipes...\n",
      "Processed 1200000 recipes...\n",
      "Processed 1210000 recipes...\n",
      "Processed 1220000 recipes...\n",
      "Processed 1230000 recipes...\n",
      "Processed 1240000 recipes...\n",
      "Processed 1250000 recipes...\n",
      "Processed 1260000 recipes...\n",
      "Processed 1270000 recipes...\n",
      "Processed 1280000 recipes...\n",
      "Processed 1290000 recipes...\n",
      "Processed 1300000 recipes...\n",
      "Processed 1310000 recipes...\n",
      "Processed 1320000 recipes...\n",
      "Processed 1330000 recipes...\n",
      "Processed 1340000 recipes...\n",
      "Processed 1350000 recipes...\n",
      "Processed 1360000 recipes...\n",
      "Processed 1370000 recipes...\n",
      "Processed 1380000 recipes...\n",
      "Processed 1390000 recipes...\n",
      "Processed 1400000 recipes...\n",
      "Processed 1410000 recipes...\n",
      "Processed 1420000 recipes...\n",
      "Processed 1430000 recipes...\n",
      "Processed 1440000 recipes...\n",
      "Processed 1450000 recipes...\n",
      "Processed 1460000 recipes...\n",
      "Processed 1470000 recipes...\n",
      "Processed 1480000 recipes...\n",
      "Processed 1490000 recipes...\n",
      "Processed 1500000 recipes...\n",
      "Processed 1510000 recipes...\n",
      "Processed 1520000 recipes...\n",
      "Processed 1530000 recipes...\n",
      "Processed 1540000 recipes...\n",
      "Processed 1550000 recipes...\n",
      "Processed 1560000 recipes...\n",
      "Processed 1570000 recipes...\n",
      "Processed 1580000 recipes...\n",
      "Processed 1590000 recipes...\n",
      "Processed 1600000 recipes...\n",
      "Processed 1610000 recipes...\n",
      "Processed 1620000 recipes...\n",
      "Processed 1630000 recipes...\n",
      "Processed 1640000 recipes...\n",
      "Processed 1650000 recipes...\n",
      "Processed 1660000 recipes...\n",
      "Processed 1670000 recipes...\n",
      "Processed 1680000 recipes...\n",
      "Processed 1690000 recipes...\n",
      "Processed 1700000 recipes...\n",
      "Processed 1710000 recipes...\n",
      "Processed 1720000 recipes...\n",
      "Processed 1730000 recipes...\n",
      "Processed 1740000 recipes...\n",
      "Processed 1750000 recipes...\n",
      "Processed 1760000 recipes...\n",
      "Processed 1770000 recipes...\n",
      "Processed 1780000 recipes...\n",
      "Processed 1790000 recipes...\n",
      "Processed 1800000 recipes...\n",
      "Processed 1810000 recipes...\n",
      "Processed 1820000 recipes...\n",
      "Processed 1830000 recipes...\n",
      "Processed 1840000 recipes...\n",
      "Processed 1850000 recipes...\n",
      "Processed 1860000 recipes...\n",
      "Processed 1870000 recipes...\n",
      "Processed 1880000 recipes...\n",
      "Processed 1890000 recipes...\n",
      "Processed 1900000 recipes...\n",
      "Processed 1910000 recipes...\n",
      "Processed 1920000 recipes...\n",
      "Processed 1930000 recipes...\n",
      "Processed 1940000 recipes...\n",
      "Processed 1950000 recipes...\n",
      "Processed 1960000 recipes...\n",
      "Processed 1970000 recipes...\n",
      "Processed 1980000 recipes...\n",
      "Processed 1990000 recipes...\n",
      "Processed 2000000 recipes...\n",
      "Processed 2010000 recipes...\n",
      "Processed 2020000 recipes...\n",
      "Processed 2030000 recipes...\n",
      "Processed 2040000 recipes...\n",
      "Processed 2050000 recipes...\n",
      "Processed 2060000 recipes...\n",
      "Processed 2070000 recipes...\n",
      "Processed 2080000 recipes...\n",
      "Processed 2090000 recipes...\n",
      "Processed 2100000 recipes...\n",
      "Processed 2110000 recipes...\n",
      "Processed 2120000 recipes...\n",
      "Processed 2130000 recipes...\n",
      "Processed 2140000 recipes...\n",
      "Processed 2150000 recipes...\n",
      "Processed 2160000 recipes...\n",
      "Processed 2170000 recipes...\n",
      "Processed 2180000 recipes...\n",
      "Processed 2190000 recipes...\n",
      "Processed 2200000 recipes...\n",
      "Processed 2210000 recipes...\n",
      "Processed 2220000 recipes...\n",
      "Processed 2230000 recipes...\n",
      "Processed 2231142 recipes...\n",
      "\n",
      "--- DONE ---\n",
      "Successfully saved 2231142 recipes to training_data.txt\n"
     ]
    }
   ],
   "source": [
    "# Define your tokens here\n",
    "VEGAN_TOKEN, NORMAL_TOKEN = \"[VEGAN]\", \"[NORMAL]\"\n",
    "TITLE_TOKEN, INGRED_TOKEN, STEPS_TOKEN = \"[TITLE]\", \"[INGRED]\", \"[STEPS]\"\n",
    "EOS_TOKEN = \"<|endoftext|>\"\n",
    "\n",
    "def create_training_data(csv_path, output_filename=\"training_data.txt\"):\n",
    "    # 1. Use 'chunksize' to load only 10,000 recipes at a time\n",
    "    # This prevents your RAM from filling up\n",
    "    reader = pd.read_csv(csv_path, chunksize=10000)\n",
    "    animal_prods = {'milk', 'eggs', 'butter', 'meat', 'beef', 'chicken', 'fish', 'cheese', 'lard'}\n",
    "    \n",
    "    count = 0\n",
    "    # Open the file once and append to it in the loop\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for chunk in reader:\n",
    "            for _, row in chunk.iterrows():\n",
    "                try:\n",
    "                    # Parse lists safely\n",
    "                    ner_list = eval(row['NER'])\n",
    "                    directions = eval(row['directions'])\n",
    "                    \n",
    "                    # Logic\n",
    "                    is_vegan = all(item not in animal_prods for item in ner_list)\n",
    "                    diet_tag = VEGAN_TOKEN if is_vegan else NORMAL_TOKEN\n",
    "                    \n",
    "                    # Build string\n",
    "                    recipe_str = (\n",
    "                        f\"{diet_tag} {TITLE_TOKEN} {row['title']} \"\n",
    "                        f\"{INGRED_TOKEN} {', '.join(ner_list)} \"\n",
    "                        f\"{STEPS_TOKEN} {' '.join(directions)} {EOS_TOKEN}\\n\"\n",
    "                    )\n",
    "                    \n",
    "                    # Write directly to disk\n",
    "                    f.write(recipe_str)\n",
    "                    count += 1\n",
    "                except:\n",
    "                    continue # Skip rows with broken data\n",
    "            \n",
    "            print(f\"Processed {count} recipes...\")\n",
    "\n",
    "    print(f\"\\n--- DONE ---\")\n",
    "    print(f\"Successfully saved {count} recipes to {output_filename}\")\n",
    "    # NO RETURN STATEMENT HERE. The data is safe on your disk.\n",
    "\n",
    "# Execute\n",
    "create_training_data('/home/onyxia/work/Recipe-Generator/data/full_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62479f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting recipes in training_data.txt...\n",
      "Total recipes found: 2,255,898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2255898"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_recipes_efficiently(filepath):\n",
    "    print(f\"Counting recipes in {filepath}...\")\n",
    "    count = 0\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # We only count lines that aren't empty\n",
    "            if line.strip():\n",
    "                count += 1\n",
    "    \n",
    "    print(f\"Total recipes found: {count:,}\")\n",
    "    return count\n",
    "\n",
    "# Usage:\n",
    "count_recipes_efficiently('training_data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e708aa",
   "metadata": {},
   "source": [
    "## 2). Tokenizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3193332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training_data.txt -> train.bin...\n",
      "Processed chunk 1 (100MB total)...\n",
      "Processed chunk 2 (200MB total)...\n",
      "Processed chunk 3 (300MB total)...\n",
      "Processed chunk 4 (400MB total)...\n",
      "Processed chunk 5 (500MB total)...\n",
      "Processed chunk 6 (600MB total)...\n",
      "Processed chunk 7 (700MB total)...\n",
      "Processed chunk 8 (800MB total)...\n",
      "Processed chunk 9 (900MB total)...\n",
      "Processed chunk 10 (1000MB total)...\n",
      "Processed chunk 11 (1100MB total)...\n",
      "Processed chunk 12 (1200MB total)...\n",
      "Processed chunk 13 (1300MB total)...\n",
      "Processed chunk 14 (1400MB total)...\n",
      "\n",
      "--- Tokenization Complete ---\n",
      "Binary file saved as: train.bin\n",
      "File size: 707.31 MB\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_save(input_file, output_bin, chunk_size_mb=100):\n",
    "    \"\"\"\n",
    "    Tokenizes text in chunks and streams directly to a binary file.\n",
    "    This ensures we don't crash RAM and the process is 100% reproducible.\n",
    "    \"\"\"\n",
    "    # 1. Initialize the tokenizer (GPT-2 encoding matches tiktoken's default)\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Error: {input_file} not found. Please run your data prep script first.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Tokenizing {input_file} -> {output_bin}...\")\n",
    "    \n",
    "    # 2. Open binary file for writing\n",
    "    with open(output_bin, 'wb') as bin_f:\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            chunk_count = 0\n",
    "            while True:\n",
    "                # Read chunks (100MB is a good balance for speed/RAM)\n",
    "                text_chunk = f.read(1024 * 1024 * chunk_size_mb)\n",
    "                if not text_chunk:\n",
    "                    break\n",
    "                \n",
    "                # Encode chunk to token IDs\n",
    "                ids = enc.encode_ordinary(text_chunk)\n",
    "                \n",
    "                # Convert to uint16 (efficiently handles vocab up to 65,535)\n",
    "                # GPT-2 vocab is 50,257, so uint16 saves 50% space vs int32\n",
    "                ids_array = np.array(ids, dtype=np.uint16)\n",
    "                \n",
    "                # Save chunk to disk immediately\n",
    "                bin_f.write(ids_array.tobytes())\n",
    "                \n",
    "                chunk_count += 1\n",
    "                print(f\"Processed chunk {chunk_count} ({chunk_size_mb * chunk_count}MB total)...\")\n",
    "\n",
    "    print(\"\\n--- Tokenization Complete ---\")\n",
    "    print(f\"Binary file saved as: {output_bin}\")\n",
    "    print(f\"File size: {os.path.getsize(output_bin) / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Run the process\n",
    "tokenize_and_save('training_data.txt', 'train.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7268039",
   "metadata": {},
   "source": [
    "## 3). Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46ba1744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader initialized. Dataset has 370,832,540 tokens.\n",
      "X shape: torch.Size([32, 512]), Y shape: torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, bin_file, batch_size, block_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # Memory-map the binary file (The Pro Move)\n",
    "        # This keeps RAM usage at nearly 0MB\n",
    "        self.data = np.memmap(bin_file, dtype=np.uint16, mode='r')\n",
    "        print(f\"DataLoader initialized. Dataset has {len(self.data):,} tokens.\")\n",
    "\n",
    "    def get_batch(self):\n",
    "        # Pick random starting points for the whole batch\n",
    "        # We leave room for block_size + 1 so we can get the target y\n",
    "        ix = torch.randint(len(self.data) - self.block_size, (self.batch_size,))\n",
    "        \n",
    "        # Pull the sequences from the memory map\n",
    "        x = torch.stack([torch.from_numpy((self.data[i:i+self.block_size]).astype(np.int64)) for i in ix])\n",
    "        y = torch.stack([torch.from_numpy((self.data[i+1:i+1+self.block_size]).astype(np.int64)) for i in ix])\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            x, y = x.pin_memory().cuda(non_blocking=True), y.pin_memory().cuda(non_blocking=True)\n",
    "            \n",
    "        return x, y\n",
    "\n",
    "train_loader = DataLoader('train.bin', batch_size, block_size)\n",
    "x, y = train_loader.get_batch()\n",
    "print(f\"X shape: {x.shape}, Y shape: {y.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
